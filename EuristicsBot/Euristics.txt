Проверить внутри границ, на границах и за границами (boundary values).
Это не нужно тестировать. Для проекта важно другое.
Нет возможности протестировать это.
Поставить на прод с расширенным логированием и посмотреть на пользователей.
Это не самое важное, начать надо с другой функциональности.
Проверить на данных реальных пользователей.
Проверить все сообщения об ошибках. Код, который работает с ошибками часто бывает написан хуже.
Проверять на площадке/машине/конфигурации не такой, как у разработчика. Хороший разработчик позаботится, чтобы хотя бы у него работало.
Начать с тестов, которые трудно или просто бесит готовить или делать. Вероятность того, что “легкие” тесты прогонят выше, чем для “тяжелых”
Не делать избыточные тесты.
Перепроверить уже проверенное - разработчик мог сломать это последними правками.
Проверить по всем пунктам постановки задачи.
Прогнать все автотесты.
Проверить, что прошлые обновления влиты в текущее.
Написать автотесты до того, как разработчик напишет код и отдать ему (test-first).
Проверить обратную совместимость, возможность отката и порядок установки частей обновления.
Почитать коммиты. Как было, как стало, как это работает.
Почитать комменты, которые были на code review.
Проверить, что разработчик какие-то тесты написал.
Прогнать нагрузочное. Посмотреть на: коды ответов, ошибки в логах, среднее, медианное и 95% времени ответа, расход памяти, процессора, сети, диска. Сравнить с цифрами прошлого прогона и прогона месяц назад.
Если в ветку обновления вливали другую ветку, посмотреть в гите, что именно вливали.
Проверить, что прошлое обновление влито в текущее.
Сценарий быстрого пользователя - очень быстро проходит сценарии, иногда жмет на кнопки несколько раз.
Сценарий пользователя, который пользуется хоткеями. Например, переход по полям формы по Tab, отправка формы по Enter.
Сценарий пользователя, который опаздывает. Например, сдает отчет, сроки по которому выходят через 15 минут.
Сценарий медленного пользователя. Делает все очень не торопясь, в любой момент может пойти попить чаю на полчаса-час. Обычно этот сценарий находит проблемы с таймаутами.
Попробовать грузить файлы: огромные, пустые, с неверным расширением, не по формату, архивы, в неверной кодировке.
Кодировка XML: поддерживаемая, неподдерживаемая, неверно указанная в декларации.
Кривые XML файлы: незакрытый тег, незакрытая кавычка в атрибуте, CDATA, кодирование не-XML символов (> должен стать &gt;), отсутствие декларации, недопустимые символы - ∞, отсутствие корневого элемента
Если есть XSD схема, проверить реакцию системы на файл, который не сооветствует xsd.
Найти всех важных для нас заинтересованных лиц обновления. Понять их желания. Убедиться, что их мечты сбудутся.
Попросить прогнать тесты тех, у кого с нами есть интеграция.
Если интеграций много, сделать тестовое облако, завязать на нее тестовые площадки других проектов и перед установкой на прод "отстаивать" обновления на облаке.
Посмотреть во всех поддерживаемые браузерах.
Начать с самого бажного браузера (например IE8).
Посмотреть в разных расширениях экранов пользователей.
Спросить у проектировщиков типажи пользователей и проверить их сценарии.
Применить Pairwise.
Поискать в багтерекере старые баги. Убедиться, что закрытые баги не вернулись. Проверить, может быть открытые баги закроются или потеряют актуальность.
Посмотреть, как фича реализована в продуктах конкурентов.
Посмотреть что все отправляется и приходит куда нужно, кому нужно и сколько нужно раз
В печатных формах проверять, что влезают поля максимальной длины.
Если в печатной форме пользователь может добавлять разделы, проверить случай, когда разделы перестали помещаться на 1 лист.
Выдрать из код и проверить в regular expressions. В регулярках часто ошибаются, а еще они иногда вызывают Out Of memory Exception.
Email еще бывает: на кириллических доменах, со знаками .,+-_
Если есть какая-то логика, затронутая изменениями, но сама не измененная. Подумать: Почему мы считаем, что эта логика работает правильно (есть ли открытые баги? проводилось ли тестирование? достаточно ли нам свидетельства "ну пользователи же не жалуются"?).
Как должна работать фича?
Где фича может сломаться? Предположить: "Мы выпустили обновление и оно сломалось. В каком месте?", чтобы отделаться от положительной предвзятости.
Поломка в каком месте будет критична для продукта?
Как будет использоваться пользователями?
Какие места важны с точки зрения заказчика?
Насторожиться, если задача заведена давно - мог измениться контекст и сейчас нужно делать что-то другое.
Что будет через полгода, год использования фичи?
Порядок установки обновления: спросить у разработчика, каким он должен быть. Спросить, какие настройки были сделаны для тестовых и должны быть сделаны для боевых.
Проверяем, что все остальное не сломалось — регрессионное тестирование.
Найди еще одного тестировщика, чтобы подумать о нужных проверках вместе. Составьте первый план вдвоем.
До написания кода провести дизайн-ревью для верификации ТЗ. Пригласить: разработчиков, тестеров, менеджеров, админов, заинтересованных лиц, умного тимлида. Рассказать о том, какую задачу решаете и какой способ выбрали. Приготовиться к замечаниям.
Спланировать, какие проверки в каких автотестах будут реализованы. Согласовать время на написание автотестов в рамках текущей фичи.
Перед выполнением тестов (руками или АТ) убедиться что code review пройдено и разработчик ничего нового добавлять не планирует.
Попытаться доказать, что ТЗ неверно. "Этот пункт аналитики неверный". "Этот пункт аналитики - неправда". И т.д.
Когда пытаешься понять сложную фичу, представить, что ты - это она. Так будет проще въехать в тему.
Выделить требования: бизнес (каких целей пользователя надо достичь), системные (что требуется от системы). Проверить их соответствие друг другу.
Убедиться в тождестве: здравый смысл = желания пользователей = какую задачу поставил менеджер = что написал аналитик =что закодировал разработчик.
Узнать, зачем аналитик/менеджер заказал фичу.
Узнать, какую задачу выполнял разработчик.
Посмотреть старые планы тестирования. Почему раньше это тестировали, а сейчас не тестируем (или наоборот)?
Посмотреть открытые баги (включая древние, покрытые пылью, не прикрепленные к обновлению)
Для понимания больших фич желательно, чтобы прошло несколько дней от начала сбора инфы до начала тестирования. Поэтому загружаться инфой нужно пораньше.
Находим зависимые сборки, убеждаемся, что их компиляция не сломалась.
Вне тестирования текущей фичи придумать и обозначить стратегию тестирования.
Определить критерии конца тестирования. Это могут быть: дедлайн, расстановка приоритетов и тестирование только первого, субъективная уверенность тестировщика.
Если багов очень много - скорее всего, их будет еще больше, потому что что-то не так в процессе разработки. Тестер на это в одиночку не повлияет - нужно идти к менеджеру.
Посмотреть во все мониторинги, что есть: ошибки в логах, графики в Zabbix, системные метрики серверов.
Тестировать в первую очередь там, где ожидаются баги.
По каждой пункту тест-плана задать вопрос: Это не нужно проверять. Почему?
Что нужно сделать после релиза? Поправить аналитику, поделиться с кем-то какой-то инфой...что еще?
При составлении тест-плана с самого начала записываем свои предположения. Через какое-то время смотрим на них и оспариваем каждое.
Если тестируешь не один: Обозначить места, где усилия можно распараллелить.
Отправить на тест-план тест-ревью или хотя бы просто побазарить с другим тестировщиком.
Верстка: выполнены ли требования по ней?
Верстка: соответствует прототипам/макетам?
Спросить аналитика: Зачем нужна фича? У нас есть SLA? Какие задачи должна выполнять фича? Что обязательно нужно проверить? Какая функциональность критическая? Кто заказчик?
Спросить разработчика: Для чего он делал фичу? Как она работает? Какие изменения он внес? Что может сломаться? Что стоит проверить? Какие тесты и где есть? Что разработчику не понравилось и понравилось в аналитике и процессе разработки фичи? В чем он уверен? В чем не уверен? Места, про которые он скажет в последних двух вопросах нужно посмотреть особенно тщательно.
Опросить самого себя. Почему ты решил, что все будет работать именно так? И только так? По каждому пункту плана тестирования "Это не правильно. Почему?".
Что будет с нашим приложением, если сломается другой сервис, от которого мы зависим? Например, перестанет отвечать или будет отвечать долго или неправильно? 
Запросы в хранилище статистики должны улетать в режиме fire and forget. Лаги или недоступность сервера статистики не должны влиять на работоспособность сервиса.
Если нашему приложению будет плохо. Как мы узнаем, где проблема (логи, мониторинги, звонки в техподдержку? Это могут быть: проблемы на железе, выжирание диска, памяти, сети, плохие коды ответов, деградация времени ответа, функциональные баги.
Еще разок почитать документацию на приложение.
Если у сервиса есть ресурсы, нужна валидация ресурсов при деплое и при работе сервиса. Один плохой файл ресурсов не должен ломать весь сервис.
Обратная совместимость: что нужно сделать, чтобы откатить сервис в случае чего и можем ли мы его откатить. Например, не сможем откатить, если обновление меняет формат хранения данных в базе.
Если сервис переписали и есть баг, стоит проверить: нет ли такого бага в старом сервисе; не завязана ли логика других сервисов на этот баг.
Если заявлено, что API нового сервиса должно совпадать со старым, то нужно сравнивать: new API = old API = документация (первое сравнение важнее)
Собрать всю информацию, которая будет нужна для деплоя.
Если на смену одному микросервису приходит другой, удобно написать Approval тесты на старый сервис и поднять их на новом.
Поискать баги, найденные и исправленные в старом сервисе. Убедиться, что в новом их не будет
Если мы убиваем старый сервис, проверить, что к нему никто не обращается. Удобно проверять выключением сервиса на интеграционной тестовой площадке.
Если на смену одному микросервису приходит другой. Убедиться, что новый выдержит нагрузку старого. Убедиться, что новый не затопит нагрузкой другие части системы.
Если на смену одному микросервису приходит другой, но старый НЕ убиваем. уУедиться, что и правда не убиваем и у админов не занесена рука над красной кнопкой. Убедиться, чтобы сервисы могут работать вместе в проде.
Если на смену одному микросервису приходит другой, но старый НЕ убиваем. Подумать, есть ли сценарии, в которых придется обновлять оба сервиса.
Если на смену одному микросервису приходит другой. Какую еще инфу о работе старого сервиса мы можем достать из аналитики, обращений, багов, статистики?
Что мы пообещали пользователю? Должны выполнить или извиниться.
С какой целью он пришел в систему? Мы ее достигнем?
Полчаса попользоваться системой, типа ты пользователь.
Посмотреть обращения пользователей или аналитику по ним.
Достичь всех целей пользователя (пройти по сценариям). Чего хочет пользователь в каждый момент? 
Следить за чувствами при тестировании. Если чувствуешь злость - с приложением что-то не так. ("я просто хочу отправить отчет, нафига мне вот это сейчас?").
В каких терминах мыслит пользователь? (например пользователь отправляет не файлы, а отчеты)
Как пользователь будет обучаться работе в системе?
Насколько тяжело въехать в работу системы?
Исходить из того, что пользователь освоит только необходимый минимум функций.
Редко используемые кнопки далеко, часто - на виду.
Пользователю хочется контролировать процесс. Он это ощутит? Что ему нужно знать, чтобы ощутить?
Как еще он может использовать наш продукт?
Негативные сценарии: неправ пользователь, неправы мы. Если обновление ломает поведение в негативных редких сценариях это тяжело поймать.
Пользователя бесит все начинать сначала. В каких случаях ему придется это делать? Можем ли мы изменить это?
Какие сценарии пользователей мы задеваем, можем ли не задевать?
Пользователь со слабым компом.
Пользователь с альтернативной ОС: Windows XP, Linux, Mac OS, Windows Server...
В чем пользователь может быть виноват. Как мы можем ему помочь.
В чем можем быть виноваты мы. Что будем делать.
Новый пользователь. Пользователь у которого чего-то нет/не заполнено.
Пользователь с медленной реакцией. Может не успеть прочитать какое-нибудь сообщение об ошибке.
Пользователь со старыми данными в старом формате.
Очень Важный Пользователь, который приносит кучу денег.
Как мы перенесем пользовательские данные и что нужно переносить?
Пользователь, который первый день за компьютером.
Как разработчики будут обращаться с выпущенной фичей? Релизить, вносить изменения? Удобно ли им будет?
Читаем все "TODO" в коде.
Если случился копипаст кода, можно проверить его, посмотрев diff.
Стараемся найти смысл для каждой правки, иногда спрашиваем у разработчика "зачем это сделано"
Если используются сторонние сборки (вроде Lucene), узнаем, что это. Узнаем про известные проблемы с ними.
Были ли затронуты известные и понятные мне части и объекты системы.
Менялось ли апи.
Разумность HTTP-ответов. Например, на некорректные запросы должен быть ответ 400 Bad Request. Стоит уточнять у разработчика, какую конфессию принял он и принимал ли вообще. Может, у нас json-rpc.
Данные. Что хранится, где хранится, какие права доступа, какие ограничение на хранение. Сколько места есть. Большой контент бывает? Пролезет?
ApiKey - есть, нету, нужен ли?
Если обновление затрагивает механизмы, которые работают с чтением ИЛИ записью пользовательских данных (например, меняется формат внутреннего представления). Необходимо тестировать случай, когда часть реплик сервиса - новая, а часть еще старая. Тестировать нужно вручную (подумать, что будет в таком случае). И автоматически - прогонять автотесты на тестовой площадке, на которой часть реплик из мастера, а часть - из обновления. Если кратко: твое обновление меняет логику работу с пользовательскими данными? Если да, то определи, нужно ли его тестировать на смешанном наборе реплик. При горячем обновлении релиз должен выглядеть так: 1. Выпускаем версию сервиса, которая пишет по-новому, а читает и по-новому и по-старому. 2. Выпускаем версию сервиса, которая пишет по-новому и читает по-новому.
Порядок установки обновления: спросить у разработчика, каким он должен быть. Спросить, какие настройки были сделаны для тестовых и должны быть сделаны для боевых. Топология, настройки nginx, рубильники фич и аутентификации. Что-то еще.
Проверить согласованность с историей. Нынешнее поведение функции соответствует ее прошлому поведению.
Проверить согласованность с вашим представлением. Функция ведет себя согласно тому, как вы себе представляете организацию продукта.
Проверить согласованность с аналогичными продуктами. Функция ведет себя подобно тому, как ведут себя такие функции в аналогах продукта.
Проверить согласованность претензий. Функция ведет себя так, как люди говорят, что она должна себя вести.
Проверить согласованность с ожиданиями пользователя. Поведение функции соответствует тому, что мы думаем, что хотят пользователи.
Проверить согласованность с продуктом. Поведение функции соответствует поведению сопоставимых функций продукта или функциональных моделей продукта.
Проверить согласованность с целями. Функция ведет себя согласно ее прямому назначению.
Если добавили новую фукнцию. Новое поле в форме, например. Подумать, где еще такая функция есть или должна быть. Например, в каких еще формах это поле есть или должно быть.
Действия в системе. Какие действия в системе делаются регулярно и что будет, если они перестанут быть регулярными? Пример: частые релизы с перезапуском сервисом маскируют утечки памяти.
Действия в системе. Какие действия в системе делаются очень редко и что будет, если они станут регулярными? Пример: редкие, но тяжелые запросы в БД.
Если все как-то очень тяжело и не получается, что может пора поменять проект/команду/компанию
